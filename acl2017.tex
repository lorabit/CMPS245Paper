%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}
\usepackage{multirow}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{An Iterative Approach for Mining Reasons from Web Arguments}

\author{Yanan Xie \\
Computer Science Department\\
University of California, Santa Cruz\\
  {\tt yaxie@ucsc.edu} \\\And
  Ziqiang Wang \\
Computer Engineering Department\\
University of California, Santa Cruz\\
  {\tt zwang232@ucsc.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
There is an increasing research interest in mining reasons from Web arguments since many research problems derived from argument mining can benefit from structured information about reasons in the Web arguments. However, being a very hard problem, mining reasons from Web arguments is still an unsolved problem. In this paper, we present a supervised approach for mining reasons from Web arguments and an iterative approach based on the previous approach. Our experiments show that our iterative approach improves the results gained by the previous approach in some cases.
\end{abstract}

\section{Introduction}

The main goal of argumentation is persuation\cite{nettel2012persuasive, mercier2011humans, blair2012argumentation}. Psychology researchers often identify that there are three factors(i.e., the argument itself, the audience of argument, the source of argument ) that affect argument persuasiveness\cite{petty1986elaboration}. 

As an important part of predicting persuasiveness of argument itself, the task of predicting convincingness started to attract researchers to work on this issue\cite{habernal2016argument}. However, existing works all stop by taking advantage of superficial features without considering context and domain knowledge. They focus on whether the argument gives examples, actual reasons and facts rather than how good the examples, reasons, facts are. Thus, those non-sense arguments containing weak or even faked supporting reasons but composed with convincing words may be automatically classified as highly convincing in existing models. In Table \ref{table:argumentexamples}, we give an example of argument pair to show that argument with reason(Argument 1) can be less convincing than one without reasons(Argument 2). It's critical to differentiate reasons of different quality while predicting convincingness. 

\begin{table}[h]
\begin{tabular}{| p{3.5cm} | p{3.5cm} | }
\hline
{\bf Argument 1} & {\bf Argument 2} \\
\hline
YES, because some children donâ€™t understand anything excect physical education especially rich children of rich parents. & Of course!  According to a research in 2018, no one can ever find his or her true love without taking physical education.\\
\hline

\end{tabular}
\caption{Argument examples. {\bf Prompt:} Should physical education be mandatory in schools? {\bf Stance:} Yes!} 
\label{table:argumentexamples}
\end{table}

In order to analyze reasons in some Web arguments, the first step is to identify reasons from the given arguments. Figure \ref{figure:reasonidentification} shows an example of reason identification. Unlike reason classification which aims at classifying reasons into some pre-labeled groups\cite{hasan2014you}, reason identification is to decide whether given argument segment is expressing a reason or not without manually-labeled knowledge about reasons. The task of reason identification is much harder than reason classification. To the best of our knowledge, there is no reported result on general reason identification for Web arguments.


\begin{figure}

{\it I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.} Let us take a look from the social perspective. {\it If parents cannot afford to provide for the child, or if the family is facing financial constraints, it is understandable that abortion can remain as one of the options.}


\caption{An example of reason identification. Text segments that are identified as reasons are {\it italicized}.} 
\label{figure:reasonidentification}
\end{figure}

In this paper, we propose a supervised approach for mining reasons from Web arguments. Experiments show that the achieved performance does not vary significantly from topics. The model trained on the same topic and the model trained on different topics perform similarly. In addition to the previous method, we also propose an iterative approach based on the previous method. Experiments show that the iterative approach achieves better performance in terms of precision and recall in many cases. 

The rest of this paper is organized as follow: Section \ref{sec:related} will give a quick review of studies in related fields. We will describe our approaches in Section \ref{sec:approach} while experimental evaluations on the proposed approaches will be shown in Section \ref{sec:evaluation}. In Section \ref{sec:discussion}, we will discuss some other aspects of this problem that we do not address in this paper. As the last part of this paper, Section \ref{sec:conclusion} will conclude this paper.



\section{Related work}
\label{sec:related}

Predicting convincingness of web arguments as a new research task has recently been studied\cite{habernal2016argument}. High-quality human annotated convincingness data of argument pair in multiple domains is also available for research\cite{habernal2016argument}. 

The classification of argument components is to classify text segments in given arguments into two types - claim and premise\cite{stab2014argumentation}. This issue has been studied by \cite{rooney2012applying,feng2011classifying,palau2009argumentation,mochales2011argumentation}. This is very similar to the task of mining reasons from Web arguments. However, the major differences between our work and those mentioned works are: 1) They were not built for Web arguments; 2) They are closed-domain or built for arguments from specific topics.

Unlike other kinds of arguments, Web arguments are usually noisy\cite{habernal2016argument}. There may be a lot of typos, informal words, characters in Web arguments. It is also hard to find a general pattern from Web arguments. All of above make mining reasons from Web arguments far more difficult comparing with from other kinds of arguments.

Open-domain reason mining can be far more difficult than close-domain reason mining. Reasons in the same topic can usually be grouped into several groups\cite{hasan2014you}. Reasons in the same group often share some mutual words, use similar ways to support a certain point. Modern classifiers can greatly benefit from this kind of features. However, cross-domain(topic) reason mining does not have this fortune. Features used to train classifier have to be re-designed and re-selected. 

Reason classification on Web arguments has recently been studied\cite{hasan2014you}. And human annotated data is available which can also be used for evaluating reason clustering. Although the task of reason classification and of reason identification are totally different, we can borrow the data from \cite{hasan2014you} to evaluate the performance of our approach. 

Recently, there is an increasing research interest in evidence mining\cite{cartright2011evidence, rinott2015show, aharoni2014benchmark}. Most of them propose to detect CDEs(Context Dependent Evidences) from arguments. Besides the difference that they do not study Web arguments, one other difference from the task of reason mining is that they focus on factual reasons, a small part of types of reasons. However, for mining reasons from Web arguments, we need to be able to identify all kinds of reasons from given Web arguments.  

\section{Our approahces}
\label{sec:approach}

In this section, we will introduce a supervised approach(the baseline method) for mining reasons from Web arguments. After we introduce an algorithm for measuring semantic similarity between two text segments from Web arguments, we will present an iterative approach based on the previous approach that utilizes the similarities between candidates. 

\subsection{Initializing Classifier}

We first introduce the feature set we use to build Initializing Classifier.

{\bf total\_idf} IDF(Inverse Document Frequency) is a measure of how much information a word provides\cite{salton1986introduction}. As we argue that reasons usually carry more information than other components do. So we sum IDFs of all words appearing in the candidate up to represent how much information the candidate provides.  

{\bf average\_idf} We also use average IDF to represent how much information the candidate provides, as we believe that words in reasons are generally more informative.

{\bf discourse\_marker} Discourse marker has been known as a strong indicator for reason mining\cite{stab2014argumentation}. Discourse markers we use in this approach include {\it however}, {\it but}, {\it because}, {\it since}, {\it hence}, and {\it as}.

{\bf numeric\_token} It has been shown that some certain types of reasons tend to contain numeric analysis\cite{rinott2015show}. We use the number of numeric token in the tokenized candidate to represent this kind of property.

{\bf contains\_quote} Factual reason usually use some certain symbols including {\it :}, {\it ''} , {\it -},{\it (}, and, {\it )}\cite{rinott2015show}.  We use a binay feature to represent wether the current candidate contains a symbol in our symbol set.

{\bf num\_quote} We also use the number of symbols in the candidate that also appears in our symbol set as a feature to complement the previous feature.

{\bf lexicon\_token} As we read some Web arguments from other dataset, we found that some certain words are frequently used to express reasons. We use the number of those tokens that appear in tokenized text segment as a feature. In order to compress the word set, we use a stemmed word set instead. The stemmed word set contains {\it studi}, {\it show}, {\it research}, {\it indic}, {\it report}, {\it scientist}, {\it know}, {\it result}, {\it evid}, {\it instanc}, {\it exampl},  {\it case},  {\it when},  {\it whi},  {\it where}, and {\it what}.

{\bf marker\_token} As we noticed that in our dataset, some text segment may express multiple reasons. Some of those reasons are also separated by discourse markers. We use the same discourse marker set in {\bf discourse\_marker} to test if the candidate contains discourse marker.

In addition to previously mentioned features, we also use common NLP features like the position of the candidate in the context, the length of the candidate in terms of characters and tokens.


\subsection{Growing Classifier}

Since Web arguments are usually short and sometimes incomplete, many features mentioned above may not be identified in all reason text segments. For example, although being a strong indicator, discourse marker may not be presence before all reason text segments. It was reported that only 26\% of the reason text segments in the RST Discourse Treebank include discourse markers\cite{marcu2002unsupervised}. Furthermore, the same reason may be phrased in many ways. Words used in those different representations can be very different. Hence, features like {\bf total\_idf}, {\bf average\_idf} and {\bf lexicon\_token} do not always work. Hence, we may need to find another way to identify those hidden reason text segments. 

It has been argued in the studies of reason classification that, with a few labels provided by topic experts, algorithms of reason classification can correctly identify and classify a big portion of those reason text segments\cite{hasan2014you}. In other words, many reason text segments share mutual meanings. 

In order to utilize this kind of property to further improve the performance of reason mining, we propose an iterative approach based on the previous method. We want to train a new classifier that exploits relations between candidates in the same topic. It`s intuitive to use similarity between candidate to express this kind of relation, as in the task of reason classification, similarity between candidate and label has been used as the main feature to detect and identify reasons\cite{hasan2014you}. 

We can simply add a new feature which is computed by the maximum semantic similarity between the measured candidate and identified reasons. However, as measured candidates are unknown in the first place. Problems raised by this approach are 1) How do we obtain the added feature during predicting? 2) How do we train this classifier?

To be able to use the new classifier(which is called Growing Classifier) to detect reasons, we need to use our previous approach to initially identify a small portion of reasons. That`s why we call the previous approach Initializing Classifier. With the initial reason set identified by Initializing Classifier, we can use Growing Classifier to detect reasons iteratively, until no reasons can be added to or removed from the reason set.

As for the issue of training, we can assume that all reasons except the current one are known. We use the maximum similarity between the current candidate and all reason segments on the same topic except itself to represent the added similarity feature.

The semantic similarity between candidates is calculated by cosine similarity between semantic vectors of two text segments. The method has been long employed as an effective method to measure similarity\cite{steinbach2000comparison}. We use average embedding vectors of words in the text segment to represent the semantic vector of text segments. The word embedding model we used in this paper is word2vec\cite{mikolov2013efficient}. Although there are many more advanced methods for measuring semantic similarity like word alignment\cite{sultan2015dls}, we choose embedding based method mainly due to its simplicity in implementing, as we do not want to address similarity computation in this paper.

We have tried many popular classifiers to train our model including logistic, SVR(Support Vector Regression) and SVM(Support Vector Machine). As a result, the SVM-based model performs the best in our evaluation dataset. We use SVM from scikit-learn library to build both Initializing Classifier and Growing Classifier.


\section{Evaluation}
\label{sec:evaluation}

In this section, we will start by introducing how we obtain the data we used to train and test our approaches. And then we will evaluate our two methods in both open-domain setting and single-domain setting. We will also show some other interesting properties of our methods. 

\subsection{Dataset}

The dataset we use is from\cite{hasan2014you} which contains approximately 2,000 Web arguments from 4 different topics with labeled reasons. After looked into the dataset, we found some annotated reasons in the same Web argument can be overlapping. To simplify the process of data preprocessing, we combine those overlapping text segments into one single text segment which is the union of all overlapping segments. 

In order to use the data for the classifying problem, we need to separate origin Web arguments into two kinds of text segments, reason segments, and non-reason segments. We simply extract annotated reasons as the reason segments and those left in the origin text and separated by reason segments are the non-reason segments. We removed all text segments containing less than 3 characters. As shown in Table \ref{tab:dataset}, the dataset we obtained from above preprocessing method is pretty balanced(52\% vs. 48\%).

\begin{table}[h]
\begin{center}
\begin{tabular}{|c||c|c|}
\hline \bf Topic & \bf Non-reason & \bf Reason\\ \hline\hline
abortion  & 895  & 921  \\
gayRights & 974  & 979  \\
obama     & 630  & 734  \\
marijuana & 754  & 946  \\
Total     & 3253 & 3580 \\ 
\hline
\end{tabular}
\end{center}
\caption{\label{tab:dataset} An overview of our preprocessed dataset.}
\end{table}

\subsection{Expanding only}

When we use Growing Classifier to further detect reasons with initial reason set, we do not only give the Growing Classifier the right to add candidates into reason set, we also give the classifier the flexibility to remove detected reasons from reason set. In Table \ref{tab:expanding}, we show the results obtained by Growing Classifier without the ability to remove reasons. By comparing the results with results from Table \ref{tab:cross-marijuana}, we can see that limiting the ability of Growing Classifier does not improve the performance in terms of precision and recall in many threshold settings. Hence, we will not limit the Growing Classifier on expanding only in following evaluations. 

\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|}
\hline \bf Threshold & \bf Precision & \bf Recall & \bf F1 score \\ \hline\hline
0.50 & 0.6121 & 0.8890 & 0.7250 \\
0.55 & 0.6845 & 0.2294 & 0.3436 \\
0.60 & 0.6829 & 0.1184 & 0.2018 \\
0.65 & 0.7188 & 0.0729 & 0.1324 \\
0.70 & 0.7288 & 0.0455 & 0.08565 \\
0.75 & 0.7200 & 0.0190 & 0.0370 \\
0.80 & 0.5556 & 0.0053 & 0.01050 \\ 
\hline
\end{tabular}
\end{center}
\caption{\label{tab:expanding} Performance of our iterative method with expanding only in the second step. Trained on {\it abortion}, {\it obama}, {\it gayRights}. Tested on {\it marijuana}.}
\end{table*}

\subsection{Cross-domain reason mining}

As shown in Table. \ref{tab:ic}, our Initializing Classifier achieves F1-score 0.6779 with threshold to control recall being 0.50. The threshold is used to detect reasons with the likelihood given by SVM. We train the classifier on 3 topics and test on another topic. The result of Growing Classifier obtained in the same setting is shown in Table. \ref{tab:gc}. As we can see two results are very similar. Note that the real performance of Growing Classifier should not be as high as shown in Table. \ref{tab:gc}, since the result of Table. \ref{tab:gc} is obtained with the assumption that we have all other reasons in reason set to calculate the maximum similarity feature. However, as shown in \ref{tab:cross-gay}, the actual result of our iterative approach surprisingly outperform Initializing Classifier and Growing Classifier(with perfect knowledge of reason set) in many threshold settings. The results tested on other dataset settings are very similar. We show our cross-domain reason mining results in Table. \ref{tab:cross-gay}, \ref{tab:cross-obama}, \ref{tab:cross-abortion}, and \ref{tab:cross-marijuana}.

\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|}
\hline \bf Threshold & \bf Precision & \bf Recall &\bf F1 score \\ \hline\hline
0.50& 0.5377    & 0.9173 & 0.6780 \\
0.55& 0.6590    & 0.4362 & 0.5249 \\
0.60& 0.6173    & 0.1532 & 0.2455 \\
0.65& 0.6405    & 0.1001 & 0.1731 \\
0.70& 0.5570    & 0.0449 & 0.08310 \\
0.75& 0.5500    & 0.0225 & 0.04323 \\
0.80& 0.5833    & 0.0072 & 0.01422 \\
0.85& 0.5000    & 0.0010 & 0.0020 \\ 
\hline
\end{tabular}
\end{center}
\caption{\label{tab:ic} Performance of our baseline method (initializing classifier). Trained on {\it abortion}, {\it obama}, {\it marijuana}. Tested on {\it gayRights}.}
\end{table*}

\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|}
\hline \bf Threshold & \bf Precision & \bf Recall &\bf F1 score \\ \hline\hline
0.50 & 0.5370 & 0.9122 & 0.6760\\
0.55 & 0.6609 & 0.4321 & 0.5225 \\
0.60 & 0.6320 & 0.1736 & 0.2724 \\
0.65 & 0.6292 & 0.1144 & 0.1936 \\
0.70 & 0.5591 & 0.0531 & 0.09699 \\
0.75 & 0.5400 & 0.0276 & 0.05252 \\
0.80 & 0.6471 & 0.0112 & 0.02202 \\
0.85 & 0.6667 & 0.0020 & 0.003988 \\
\hline
\end{tabular}
\end{center}
\caption{\label{tab:gc} Performance of growing classifier. Assume we know other reasons in the same topic when calculating similarity-based feature. Trained on {\it abortion}, {\it obama}, {\it marijuana}. Tested on {\it gayRights}.}
\end{table*}


\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\bf Threshold}} & \multicolumn{2}{c|}{\bf Baseline}  & \multicolumn{3}{c|}{\bf Our Method}    \\ \cline{2-6} 
\multicolumn{1}{|c|}{}   & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{l|}{\bf Recall} & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{c|}{\bf Recall} & \multicolumn{1}{l|}{\bf Iterations} \\ \hline
 \hline
0.50     & \bf 0.5377& \bf0.9173   & 0.5370 & 0.9122    & 2 \\
0.55     & 0.6590& \bf0.4362   &\bf 0.6609 & 0.4321    & 2 \\
0.60     & 0.6173& 0.1532   &\bf 0.6320 &\bf 0.1736    & 2 \\
0.65     &\bf 0.6405& 0.1001   & 0.6292 & \bf0.1144    & 2 \\
0.70     & 0.5570& 0.0449   &\bf 0.5591 &\bf 0.0531    & 2 \\
0.75     &\bf 0.5500& 0.0225   & 0.5306 &\bf 0.0266    & 2 \\
0.80     & 0.5833& 0.0072   &\bf 0.6250 &\bf 0.0102    & 2 \\
0.85     & 0.5000& 0.0010   &\bf 0.6667 &\bf 0.0020    & 2
\\\hline
\end{tabular}
\end{center}
\caption{\label{tab:cross-gay} Performance of our iterative approach with open-domain experiment setting. Trained on {\it abortion}, {\it obama}, {\it marijuana}. Tested on {\it gayRights}. (Bold values are better.)}
\end{table*}

\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\bf Threshold}} & \multicolumn{2}{c|}{\bf Baseline}  & \multicolumn{3}{c|}{\bf Our Method}    \\ \cline{2-6} 
\multicolumn{1}{|c|}{}   & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{l|}{\bf Recall} & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{c|}{\bf Recall} & \multicolumn{1}{l|}{\bf Iterations} \\ \hline
 \hline
0.50 &\bf 0.5741 &\bf 0.8338 & 0.5737 & 0.8270 & 2 \\
0.55 &\bf 0.7357 & 0.2275 & 0.7287 &\bf 0.2561 & 2 \\
0.60 & 0.7130 & 0.1117 &\bf 0.7313 &\bf 0.1335 & 2 \\
0.65 &\bf 0.6944 & 0.0681 & 0.6875 &\bf 0.0749 & 2 \\
0.70 & 0.6444 & 0.0395 &\bf 0.6604 &\bf 0.0477 & 2 \\
0.75 & 0.5417 & 0.0177 &\bf 0.5882 &\bf 0.0272 & 2 \\
0.80 &\bf 0.7273 & 0.0109 & 0.6429 &\bf 0.0123 & 2
\\\hline
\end{tabular}
\end{center}
\caption{\label{tab:cross-obama} Performance of our iterative approach with open-domain experiment setting. Trained on {\it abortion}, {\it gayRights}, {\it marijuana}. Tested on {\it obama}. (Bold values are better.)}
\end{table*}

\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\bf Threshold}} & \multicolumn{2}{c|}{\bf Baseline}  & \multicolumn{3}{c|}{\bf Our Method}    \\ \cline{2-6} 
\multicolumn{1}{|c|}{}   & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{l|}{\bf Recall} & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{c|}{\bf Recall} & \multicolumn{1}{l|}{\bf Iterations} \\ \hline
 \hline
0.50 &\bf 0.5341 &\bf 0.9001 & 0.5336 & 0.8969 & 2 \\
0.55 & 0.5402 &\bf 0.8382 &\bf 0.5418 & 0.8371 & 2 \\
0.60 & 0.6104 & 0.1650 &\bf 0.6330 &\bf 0.1835 & 2 \\
0.65 &\bf 0.5758 & 0.0825 & 0.5621 &\bf 0.0934 & 2 \\
0.70 & 0.5775 & 0.0445 &\bf 0.5789 &\bf 0.0478 & 2 \\
0.75 &\bf 0.6364 & 0.0228 & 0.6341 &\bf 0.0282 & 2 \\
0.80 & 0.6667 & 0.0065 &\bf 0.7143 &\bf 0.0109 & 2 \\
0.85 & 0.6667 & 0.0022 & 0.6667 & 0.0022 & 1
\\\hline
\end{tabular}
\end{center}
\caption{\label{tab:cross-abortion} Performance of our iterative approach with open-domain experiment setting. Trained on {\it obama}, {\it gayRights}, {\it marijuana}. Tested on {\it abortion}. (Bold values are better.)}
\end{table*}

\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\bf Threshold}} & \multicolumn{2}{c|}{\bf Baseline}  & \multicolumn{3}{c|}{\bf Our Method}    \\ \cline{2-6} 
\multicolumn{1}{|c|}{}   & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{l|}{\bf Recall} & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{c|}{\bf Recall} & \multicolumn{1}{l|}{\bf Iterations} \\ \hline
 \hline
0.50 & 0.6095 &\bf 0.8911 &\bf 0.6124 & 0.8869 & 2 \\
0.55 & 0.6823 & 0.2156 &\bf 0.6900 &\bf 0.2400 & 2 \\
0.60 &\bf 0.6687 & 0.1131 & 0.6667 &\bf 0.1205 & 2 \\
0.65 & 0.7126 & 0.0655 &\bf 0.7404 &\bf 0.0814 & 2 \\
0.70 & 0.7069 & 0.0433 &\bf 0.7164 &\bf 0.0507 & 2 \\
0.75 & 0.6957 & 0.0169 &\bf 0.7879 &\bf 0.0275 & 2 \\
0.80 & 0.6000 & 0.0063 &\bf 0.6923 &\bf 0.0095 & 2
\\\hline
\end{tabular}
\end{center}
\caption{\label{tab:cross-marijuana} Performance of our iterative approach with open-domain experiment setting. Trained on {\it obama}, {\it gayRights}, {\it abortion}. Tested on {\it marijuana}. (Bold values are better.)}
\end{table*}


\subsection{Same-domain reason mining}

Since there are already a lot of work done in reason mining for some specific domains like law\cite{palau2009argumentation}. We do not want to build another domain specific classifier. So we also evaluate our approaches in the single-domain setting for some topic. Results shown in Table \ref{tab:same-topic} show that our approaches do not bias close-domain tasks. 

\begin{table*}[h]
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|}
\hline
\multicolumn{1}{|c|}{\multirow{2}{*}{\bf Threshold}} & \multicolumn{2}{c|}{\bf Baseline}  & \multicolumn{3}{c|}{\bf Our Method}    \\ \cline{2-6} 
\multicolumn{1}{|c|}{}   & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{l|}{\bf Recall} & \multicolumn{1}{l|}{\bf Precision} & \multicolumn{1}{c|}{\bf Recall} & \multicolumn{1}{l|}{\bf Iterations} \\ \hline
 \hline
0.50 & 0.5169 & 0.9079 & 0.5172 & 0.8904 & 2 \\
0.55 & 0.5209 & 0.8202 & 0.6691 & 0.2039 & 2 \\
0.60 & 0.6167 & 0.0811 & 0.6207 & 0.0789 & 2 \\
0.65 & 0.5882 & 0.0439 & 0.6216 & 0.0504 & 2 \\
0.70 & 0.5652 & 0.0285 & 0.5833 & 0.0307 & 2 \\
0.75 & 0.6000 & 0.0197 & 0.6111 & 0.0241 & 2 \\
0.80 & 0.6364 & 0.0154 & 0.6364 & 0.0154 & 1 \\
0.85 & 0.7500 & 0.0132 & 0.7500 & 0.0132 & 1
\\\hline
\end{tabular}
\end{center}
\caption{\label{tab:same-topic} Performance of our iterative approach with close-domain experiment setting. Trained on {\it gayRights}. Tested on {\it gayRights}. (Bold values are better.)}
\end{table*}




\section{Discussion}
\label{sec:discussion}

Recently, RNN(Recurrent Neural Network)-based models have been proved success in many NLP tasks\cite{mikolov2010recurrent,luong2015effective}. RNN-based methods benefit from end-to-end training and the ability to catch the sequential features of natural language. We believe there must be some RNN-based methods that can achieve better results than ours. 

Another issue of this paper is that the dataset we use is not big enough for fully demonstrating the improvements made by the proposed approach. We are expecting the presence of other larger datasets for evaluating reason mining approaches.

Text segmentation is not addressed in this paper, as there are many papers on exploiting sentence completeness to split text for evidence mining that we can directly borrow from\ref{rinott2015show}.

\section{Conclusion}
\label{sec:conclusion}

As a fundamental problem to many argumentation mining tasks, open-domain reason mining is indeed a hard problem. In this paper, we build two classifiers to attack the problem of reason mining. Our first approach achieves an F1-score of 0.6779 while the second approach improves the performance based on the previous method in most threshold settings. Future work on this topic may require a better dataset.

\section*{Acknowledgments}

We appreciate the help from Ruiming Wang for suggesting us to use SVM rather than SVR to build the classifier. 

% include your own bib file like this:
% \bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{acl2017}
\bibliographystyle{acl_natbib}


\end{document}
