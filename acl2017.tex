%
% File acl2017.tex
%
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2017}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B{\sc ib}\TeX}

\title{An Iterative Approach for Mining Reasons from Web Arguments}

\author{Yanan Xie \\
Computer Science Department\\
University of California, Santa Cruz\\
  {\tt yaxie@ucsc.edu} \\\And
  Ziqiang Wang \\
Computer Engineering Department\\
University of California, Santa Cruz\\
  {\tt zwang232@ucsc.edu} \\}

\date{}

\begin{document}
\maketitle
\begin{abstract}
There is an increasing research interest on mining reasons from Web arguments, since many research problems derived from argument mining can benefit from structured information about reasons in the Web arguments. However, being a very hard problem, mining reasons from Web arguments is still an unsolved problem. In this paper, we present a supervised approach for mining reasons from Web arguments and an iterative approach based on the previous approach. Our experiments show that our iterative approach improves the results gained by the previous approach in some cases.
\end{abstract}

\section{Introduction}

The main goal of argumentation is persuation\cite{nettel2012persuasive, mercier2011humans, blair2012argumentation}. Psychology researchers often identify that there are three factors(i.e., the argument itself, the audience of argument, the source of argument ) that affect argument persuasiveness\cite{petty1986elaboration}. 

As an important part of predicting persuasiveness of argument itself, the task of predicting convincingness started to attrack researchers to work on this issue\cite{habernal2016argument}. However, existing works all stop by taking advantage of superficial features without considering context and domain knowledge. They focus on wether the argument gives exmaples, actual reasons and facts rather than how good the examples, reasons, facts are. Thus, those non-sense arguments containing weak or even faked supporting reasons but composed with convincing words may be automatically classified as highly convincing in existing models. In Table \ref{table:argumentexamples}, we give an exmaple of argument pair to show that argument with reason(Argument 1) can be less convincing than one without reasons(Argument 2). It's critical to differentiate reasons of different quality while predicting convincingness. 

\begin{table}[h]
\begin{tabular}{| p{3.5cm} | p{3.5cm} | }
\hline
{\bf Argument 1} & {\bf Argument 2} \\
\hline
YES, because some children donâ€™t understand anything excect physical education especially rich children of rich parents. & Of course!  According to a research in 2018, no one can ever find his or her true love without taking physical education.\\
\hline

\end{tabular}
\caption{Argument examples. {\bf Prompt:} Should physical education be mandatory in schools? {\bf Stance:} Yes!} 
\label{table:argumentexamples}
\end{table}

In order to analyze reasons in some Web arguments, the first step is to identify reasons from the given arguments. Fig. \ref{figure:reasonidentification} shows an example of reason identification. Unlike reason classification which aims on classifying reasons into some pre-labeled groups\cite{hasan2014you}, reason identification is to decide wether given argument segment is expressing a reason or not without manually-labeled knowledge about reasons. The task of reason identification is much harder than reason classification. To the best of our knowledge, there is no reported result on general reason identification for Web arguments.


\begin{figure}

{\it I feel that abortion should remain legal, or rather, parents should have the power to make the decision themselves and not face any legal hindrance of any form.} Let us take a look from the social perspective. {\it If parents cannot afford to provide for the child, or if the family is facing financial constraints, it is understandable that abortion can remain as one of the options.}


\caption{An example of reason identification. Text segments that are identified as reasons are {\it italicized}.} 
\label{figure:reasonidentification}
\end{figure}

In this paper, we propose a supervised approach for mining reasons from Web arguments. Experiments show that the achieved performance does not vary significantly from topics. The model trained on same topic and the model trained on different topics perform similarly. In addition to the previous method, we also propose an iterative approach based on the previous method. Experiments show that the iterative approach achieves better performance in terms of precision and recall in many cases. 



\section{Related work}

Predicting convincingness of web arguments as a new research task has recently been studied\cite{habernal2016argument}. High quality human annotated convincingness data of argument pair in multiple domains is also available for research\cite{habernal2016argument}. 

The classification of argument components is to classify text segments in given arguments into two types - claim and premise\cite{stab2014argumentation}. This issue has been studied by \cite{rooney2012applying,feng2011classifying,palau2009argumentation,mochales2011argumentation}. This is very similar to the task of mining reasons from Web arguments. However, the major differences between our work and those mentioned works are: 1) They were not built for Web arguments; 2) They are closed-domain or built for arguments from specific topics.

Unlike other kinds of arguments, Web arguments are usually noisy\cite{habernal2016argument}. There may be a lot of typos, informal words, characters in Web arguments. It is also hard to find a general pattern from Web arguments. All of above make mining reasons from Web arguments far more difficult comparing with from other kinds of arguments.

Open-domain reason mining can be far more difficult than close-domain reason mining. Reasons in the same topic can usually be grouped into several groups\cite{hasan2014you}. Reasons in the same group often share some mutual words, use similar way to support a certain point. Modern classifier can greatly benefits from this kind of features. However, cross-domain(topic) reason mining does not have this fortune. Features used to train classifier have to be re-designed and re-selected. 

Reason classification on Web arguments has recently been studied\cite{hasan2014you}. And human annotated data is available which can also be used for evaluating reason clustering. Although the task of reason classification and of reason identification are totally different, we can borrow the data from \cite{hasan2014you} to evaluate the performance of our approach. 

Recently, there is an increasing research interest in evidence mining\cite{cartright2011evidence, rinott2015show, aharoni2014benchmark}. Most of them propose to detect CDEs(Context Dependent Evidences) from arguments. Besides the difference that they do not study Web arguments, one other difference from the task of reason mining is that they focus on factual reasons, a small part of types of reasons. However, for mining reasons from Web arguments, we need to be able to identify all kinds of reasons from given Web arguments.  

\section{Our approahces}
\label{sec:approach}

Our proposed method identifies text segments expressing reasons from argument as the first step. An undirected graph based on similarities between reason pairs will be built for each topic. Then we use a PageRank-like algorithm to rank reasons from the generated graph while the final step is to combine reason scores with lexicon features from argument to produce the convincingness score for each argument.  

\subsection{Initializing Classifier}

Reason identification usually take advantage of discourse markers\cite{palau2009argumentation}. However, it has been reported that only few of reasons include discourse markers\cite{marcu2002unsupervised}. Given enough arguments in a topic, we can use a model based on discourse markers to select a small part of reasons, and then use reason classification techniques to identify reasons that include no discourse markers.

\subsection{Reason similarity}

Word alignment has been proven to be a powerful method to measure semantic similarity between two sentences\cite{sultan2015dls}. We are going to use a similar technique to measure similarity between two text segements identified as reasons.

\subsection{Growing Classifier}

We can easily add the computed reason scores into the feature set of \cite{habernal2016argument} and use the same classifier, Support Vector Machine to combine reason level feature with lexicon features. 




\section{Evaluation}
\label{sec:evaluation}

With the pairwise dataset, we can first evaluate our method in pair to see the precision. Also, we can further evaluate the convincingness order among given arguments by how many human labeled pair evaluations can match the algorithm produced order. 

\subsection{Dataset}

The ground truth for convincingness we choose is from \cite{habernal2016argument}. The dataset contains 16,081 argument paris from 32 different topics. 

To train our argument component classifier, we are going to use the data from \cite{hasan2014you} which contains 4 different topics with labeled reasons. However, the only overlapping topic between two datasets is gay marriage. So we will try to train a close-domain model on gay marriage topic and a open-domain model. 

\subsection{Cross-domain reason mining}

\subsection{Same-domain reason mining}


\section{Discussion}
\label{sec:discussion}

\section{Conclusion}
\label{sec:conclusion}


\section*{Acknowledgments}

We appreciate the help from Ruiming Wang for suggesting us to use SVM rather than SVR to build the classifier. 

% include your own bib file like this:
% \bibliographystyle{acl}
%\bibliography{acl2017}
\bibliography{acl2017}
\bibliographystyle{acl_natbib}


\end{document}
